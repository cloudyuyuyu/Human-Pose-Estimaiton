layer {
  name: "data"
  type: "CPMData"
  top: "data"
  top: "label"
  transform_param {
    stride: 8
    max_rotate_degree: 40.0
    crop_size_x: 368
    crop_size_y: 368
    scale_prob: 1.0
    scale_min: 0.699999988079
    scale_max: 1.29999995232
    target_dist: 1.17100000381
    center_perterb_max: 0.0
    do_clahe: false
  }
  cpmdata_param {
    source: "/mnt/qiuyu/convolutional-pose-machines-release-master/training/data1/CPM/lmdb/MPI_train_split"
    batch_size: 7
    backend: LMDB
  }
}
layer {
  name: "label_lower"
  type: "Slice"
  bottom: "label"
  top: "label_lower"
  top: "label_1st_lower"
  slice_param {
    slice_point: 15
    axis: 1
  }
}
layer {
  name: "image"
  type: "Slice"
  bottom: "data"
  top: "image"
  top: "center_map"
  slice_param {
    slice_point: 3
    axis: 1
  }
}
layer {
  name: "pool_center_lower"
  type: "Pooling"
  bottom: "center_map"
  top: "pool_center_lower"
  pooling_param {
    pool: AVE
    kernel_size: 9
    stride: 8
  }
}


#######################################
#-----------get local feature----------
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "image"
  top: "conv1"
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu_conv1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire2/squeeze1x1"
  type: "Convolution"
  bottom: "pool1"
  top: "fire2/squeeze1x1"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire2/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire2/squeeze1x1"
  top: "fire2/squeeze1x1"
}
layer {
  name: "fire2/expand1x1"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "fire2/expand1x1"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire2/relu_expand1x1"
  type: "ReLU"
  bottom: "fire2/expand1x1"
  top: "fire2/expand1x1"
}
layer {
  name: "fire2/expand3x3"
  type: "Convolution"
  bottom: "fire2/squeeze1x1"
  top: "fire2/expand3x3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire2/relu_expand3x3"
  type: "ReLU"
  bottom: "fire2/expand3x3"
  top: "fire2/expand3x3"
}
layer {
  name: "fire2/concat"
  type: "Concat"
  bottom: "fire2/expand1x1"
  bottom: "fire2/expand3x3"
  top: "fire2/concat"
}
layer {
  name: "fire3/squeeze1x1"
  type: "Convolution"
  bottom: "fire2/concat"
  top: "fire3/squeeze1x1"
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire3/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire3/squeeze1x1"
  top: "fire3/squeeze1x1"
}
layer {
  name: "fire3/expand1x1"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "fire3/expand1x1"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire3/relu_expand1x1"
  type: "ReLU"
  bottom: "fire3/expand1x1"
  top: "fire3/expand1x1"
}
layer {
  name: "fire3/expand3x3"
  type: "Convolution"
  bottom: "fire3/squeeze1x1"
  top: "fire3/expand3x3"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire3/relu_expand3x3"
  type: "ReLU"
  bottom: "fire3/expand3x3"
  top: "fire3/expand3x3"
}
layer {
  name: "fire3/concat"
  type: "Concat"
  bottom: "fire3/expand1x1"
  bottom: "fire3/expand3x3"
  top: "fire3/concat"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "fire3/concat"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire4/squeeze1x1"
  type: "Convolution"
  bottom: "pool3"
  top: "fire4/squeeze1x1"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire4/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire4/squeeze1x1"
  top: "fire4/squeeze1x1"
}
layer {
  name: "fire4/expand1x1"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "fire4/expand1x1"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire4/relu_expand1x1"
  type: "ReLU"
  bottom: "fire4/expand1x1"
  top: "fire4/expand1x1"
}
layer {
  name: "fire4/expand3x3"
  type: "Convolution"
  bottom: "fire4/squeeze1x1"
  top: "fire4/expand3x3"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire4/relu_expand3x3"
  type: "ReLU"
  bottom: "fire4/expand3x3"
  top: "fire4/expand3x3"
}
layer {
  name: "fire4/concat"
  type: "Concat"
  bottom: "fire4/expand1x1"
  bottom: "fire4/expand3x3"
  top: "fire4/concat"
}
layer {
  name: "fire5/squeeze1x1"
  type: "Convolution"
  bottom: "fire4/concat"
  top: "fire5/squeeze1x1"
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire5/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire5/squeeze1x1"
  top: "fire5/squeeze1x1"
}
layer {
  name: "fire5/expand1x1"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "fire5/expand1x1"
  convolution_param {
    num_output: 128
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire5/relu_expand1x1"
  type: "ReLU"
  bottom: "fire5/expand1x1"
  top: "fire5/expand1x1"
}
layer {
  name: "fire5/expand3x3"
  type: "Convolution"
  bottom: "fire5/squeeze1x1"
  top: "fire5/expand3x3"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire5/relu_expand3x3"
  type: "ReLU"
  bottom: "fire5/expand3x3"
  top: "fire5/expand3x3"
}
layer {
  name: "fire5/concat"
  type: "Concat"
  bottom: "fire5/expand1x1"
  bottom: "fire5/expand3x3"
  top: "fire5/concat"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "fire5/concat"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fire6/squeeze1x1"
  type: "Convolution"
  bottom: "pool5"
  top: "fire6/squeeze1x1"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire6/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire6/squeeze1x1"
  top: "fire6/squeeze1x1"
}
layer {
  name: "fire6/expand1x1"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "fire6/expand1x1"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire6/relu_expand1x1"
  type: "ReLU"
  bottom: "fire6/expand1x1"
  top: "fire6/expand1x1"
}
layer {
  name: "fire6/expand3x3"
  type: "Convolution"
  bottom: "fire6/squeeze1x1"
  top: "fire6/expand3x3"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire6/relu_expand3x3"
  type: "ReLU"
  bottom: "fire6/expand3x3"
  top: "fire6/expand3x3"
}
layer {
  name: "fire6/concat"
  type: "Concat"
  bottom: "fire6/expand1x1"
  bottom: "fire6/expand3x3"
  top: "fire6/concat"
}
layer {
  name: "fire7/squeeze1x1"
  type: "Convolution"
  bottom: "fire6/concat"
  top: "fire7/squeeze1x1"
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire7/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire7/squeeze1x1"
  top: "fire7/squeeze1x1"
}
layer {
  name: "fire7/expand1x1"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "fire7/expand1x1"
  convolution_param {
    num_output: 192
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire7/relu_expand1x1"
  type: "ReLU"
  bottom: "fire7/expand1x1"
  top: "fire7/expand1x1"
}
layer {
  name: "fire7/expand3x3"
  type: "Convolution"
  bottom: "fire7/squeeze1x1"
  top: "fire7/expand3x3"
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire7/relu_expand3x3"
  type: "ReLU"
  bottom: "fire7/expand3x3"
  top: "fire7/expand3x3"
}
layer {
  name: "fire7/concat"
  type: "Concat"
  bottom: "fire7/expand1x1"
  bottom: "fire7/expand3x3"
  top: "fire7/concat"
}
layer {
  name: "fire8/squeeze1x1"
  type: "Convolution"
  bottom: "fire7/concat"
  top: "fire8/squeeze1x1"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire8/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire8/squeeze1x1"
  top: "fire8/squeeze1x1"
}
layer {
  name: "fire8/expand1x1"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "fire8/expand1x1"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire8/relu_expand1x1"
  type: "ReLU"
  bottom: "fire8/expand1x1"
  top: "fire8/expand1x1"
}
layer {
  name: "fire8/expand3x3"
  type: "Convolution"
  bottom: "fire8/squeeze1x1"
  top: "fire8/expand3x3"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire8/relu_expand3x3"
  type: "ReLU"
  bottom: "fire8/expand3x3"
  top: "fire8/expand3x3"
}
layer {
  name: "fire8/concat"
  type: "Concat"
  bottom: "fire8/expand1x1"
  bottom: "fire8/expand3x3"
  top: "fire8/concat"
}
layer {
  name: "fire9/squeeze1x1"
  type: "Convolution"
  bottom: "fire8/concat"
  top: "fire9/squeeze1x1"
  convolution_param {
    num_output: 64
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire9/relu_squeeze1x1"
  type: "ReLU"
  bottom: "fire9/squeeze1x1"
  top: "fire9/squeeze1x1"
}
layer {
  name: "fire9/expand1x1"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "fire9/expand1x1"
  convolution_param {
    num_output: 256
    kernel_size: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire9/relu_expand1x1"
  type: "ReLU"
  bottom: "fire9/expand1x1"
  top: "fire9/expand1x1"
}
layer {
  name: "fire9/expand3x3"
  type: "Convolution"
  bottom: "fire9/squeeze1x1"
  top: "fire9/expand3x3"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "fire9/relu_expand3x3"
  type: "ReLU"
  bottom: "fire9/expand3x3"
  top: "fire9/expand3x3"
}
layer {
  name: "fire9/concat"
  type: "Concat"
  bottom: "fire9/expand1x1"
  bottom: "fire9/expand3x3"
  top: "fire9/concat"
}
layer {
  name: "drop9"
  type: "Dropout"
  bottom: "fire9/concat"
  top: "fire9/concat"
  dropout_param {
    dropout_ratio: 0.5
  }
}

layer {
  name: "local_feat"
  type: "Convolution"
  bottom: "fire9/concat"
  top: "conv4_4_CPM"
  param {
    lr_mult: 4.0
    decay_mult: 1
  }

  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
   bias_term:false
  }
}
layer {
    name: "bn_local_feat"
    type: "BatchNorm"
    bottom: "conv4_4_CPM"
    top: "conv4_4_CPM" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_local_feat"
    type: "Scale"
    bottom: "conv4_4_CPM"
    top: "conv4_4_CPM"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "local_feat_relu"
  type: "ReLU"
  bottom: "conv4_4_CPM"
  top: "conv4_4_CPM"
}

#------------------------------use this feature----------------------------------
#################################################################################
#------------------------------stage 1-------------------------------------------
#---------------fire1------------------
layer {
  name: "conv1_block1_stage1"
  type: "Convolution"
  bottom: "conv4_4_CPM"
  top: "conv1_block1_stage1"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block1_stage1"
    type: "BatchNorm"
    bottom: "conv1_block1_stage1"
    top: "conv1_block1_stage1" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block1_stage1"
    type: "Scale"
    bottom: "conv1_block1_stage1"
    top: "conv1_block1_stage1"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block1_stage1"
  type: "ReLU"
  bottom: "conv1_block1_stage1"
  top: "conv1_block1_stage1"
}


layer {
  name: "conv2_block1_stage1"
  type: "Convolution"
  bottom: "conv1_block1_stage1"
  top: "conv2_block1_stage1"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block1_stage1"
    type: "BatchNorm"
    bottom: "conv2_block1_stage1"
    top: "conv2_block1_stage1" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block1_stage1"
    type: "Scale"
    bottom: "conv2_block1_stage1"
    top: "conv2_block1_stage1"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block1_stage1"
  type: "ReLU"
  bottom: "conv2_block1_stage1"
  top: "conv2_block1_stage1"
}

layer {
  name: "conv3_block1_stage1"
  type: "Convolution"
  bottom: "conv2_block1_stage1"
  top: "conv3_block1_stage1"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block1_stage1"
    type: "BatchNorm"
    bottom: "conv3_block1_stage1"
    top: "conv3_block1_stage1" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block1_stage1"
    type: "Scale"
    bottom: "conv3_block1_stage1"
    top: "conv3_block1_stage1"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block1_stage1"
  type: "ReLU"
  bottom: "conv3_block1_stage1"
  top: "conv3_block1_stage1"
}


layer {
  name: "concat_block1_stage1"
  type: "Concat"
  bottom: "conv1_block1_stage1"
  bottom: "conv2_block1_stage1"
  bottom: "conv3_block1_stage1"
  top: "concat_block1_stage1"
  concat_param {
    axis: 1
  }
}

layer {
  name: "conv6_stage1"
  type: "Convolution"
  bottom: "concat_block1_stage1"
  top: "conv6_stage1"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv6_stage1"
    type: "BatchNorm"
    bottom: "conv6_stage1"
    top: "conv6_stage1" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv6_stage1"
    type: "Scale"
    bottom: "conv6_stage1"
    top: "conv6_stage1"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu_conv6_stage1"
  type: "ReLU"
  bottom: "conv6_stage1"
  top: "conv6_stage1"
}

layer {
  name: "conv7_stage1"
  type: "Convolution"
  bottom: "conv6_stage1"
  top: "conv7_stage1"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
   param {
    lr_mult: 8.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 15
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
      std:0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
  name: "loss_stage1"
  type: "EuclideanLoss"
  bottom: "conv7_stage1"
  bottom: "label_1st_lower"
  top: "loss_stage1"
  loss_weight:0.5
}

#################################################################################
#--------------------------stage 2-----------------------------------------------
layer {
  name: "concat_stage2"
  type: "Concat"
  bottom: "conv4_4_CPM"
  bottom: "conv7_stage1"
  bottom: "pool_center_lower"
  top: "concat_stage2"
  concat_param {
    axis: 1
  }
}

layer {
  name: "conv_stage2"
  type: "Convolution"
  bottom: "concat_stage2"
  top: "conv_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv_stage2"
    type: "BatchNorm"
    bottom: "conv_stage2"
    top: "conv_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1"
    type: "Scale"
    bottom: "conv_stage2"
    top: "conv_stage2"
    scale_param {
        bias_term: true
    }
}
layer {
  name: "relu_stage2"
  type: "ReLU"
  bottom: "conv_stage2"
  top: "conv_stage2"
}

#------------------block1----------------------
layer {
  name: "conv1_block1_stage2"
  type: "Convolution"
  bottom: "conv_stage2"
  top: "conv1_block1_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block1_stage2"
    type: "BatchNorm"
    bottom: "conv1_block1_stage2"
    top: "conv1_block1_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block1_stage2"
    type: "Scale"
    bottom: "conv1_block1_stage2"
    top: "conv1_block1_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block1_stage2"
  type: "ReLU"
  bottom: "conv1_block1_stage2"
  top: "conv1_block1_stage2"
}


layer {
  name: "conv2_block1_stage2"
  type: "Convolution"
  bottom: "conv1_block1_stage2"
  top: "conv2_block1_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block1_stage2"
    type: "BatchNorm"
    bottom: "conv2_block1_stage2"
    top: "conv2_block1_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block1_stage2"
    type: "Scale"
    bottom: "conv2_block1_stage2"
    top: "conv2_block1_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block1_stage2"
  type: "ReLU"
  bottom: "conv2_block1_stage2"
  top: "conv2_block1_stage2"
}

layer {
  name: "conv3_block1_stage2"
  type: "Convolution"
  bottom: "conv2_block1_stage2"
  top: "conv3_block1_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block1_stage2"
    type: "BatchNorm"
    bottom: "conv3_block1_stage2"
    top: "conv3_block1_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block1_stage2"
    type: "Scale"
    bottom: "conv3_block1_stage2"
    top: "conv3_block1_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block1_stage2"
  type: "ReLU"
  bottom: "conv3_block1_stage2"
  top: "conv3_block1_stage2"
}


layer {
  name: "concat_block1_stage2"
  type: "Concat"
  bottom: "conv1_block1_stage2"
  bottom: "conv2_block1_stage2"
  bottom: "conv3_block1_stage2"
  top: "concat_block1_stage2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block1_stage2"
  type: "Eltwise"
  bottom: "conv_stage2"
  bottom: "concat_block1_stage2"
  top: "block1_EltAdd_stage2"
}

#--------------block2----------------------------------
layer {
  name: "conv1_block2_stage2"
  type: "Convolution"
  bottom: "block1_EltAdd_stage2"
  top: "conv1_block2_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block2_stage2"
    type: "BatchNorm"
    bottom: "conv1_block2_stage2"
    top: "conv1_block2_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block2_stage2"
    type: "Scale"
    bottom: "conv1_block2_stage2"
    top: "conv1_block2_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block2_stage2"
  type: "ReLU"
  bottom: "conv1_block2_stage2"
  top: "conv1_block2_stage2"
}


layer {
  name: "conv2_block2_stage2"
  type: "Convolution"
  bottom: "conv1_block2_stage2"
  top: "conv2_block2_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block2_stage2"
    type: "BatchNorm"
    bottom: "conv2_block2_stage2"
    top: "conv2_block2_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block2_stage2"
    type: "Scale"
    bottom: "conv2_block2_stage2"
    top: "conv2_block2_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block2_stage2"
  type: "ReLU"
  bottom: "conv2_block2_stage2"
  top: "conv2_block2_stage2"
}

layer {
  name: "conv3_block2_stage2"
  type: "Convolution"
  bottom: "conv2_block2_stage2"
  top: "conv3_block2_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block2_stage2"
    type: "BatchNorm"
    bottom: "conv3_block2_stage2"
    top: "conv3_block2_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block2_stage2"
    type: "Scale"
    bottom: "conv3_block2_stage2"
    top: "conv3_block2_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block2_stage2"
  type: "ReLU"
  bottom: "conv3_block2_stage2"
  top: "conv3_block2_stage2"
}


layer {
  name: "concat_block2_stage2"
  type: "Concat"
  bottom: "conv1_block2_stage2"
  bottom: "conv2_block2_stage2"
  bottom: "conv3_block2_stage2"
  top: "concat_block2_stage2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block2_stage2"
  type: "Eltwise"
  bottom: "concat_block2_stage2"
  bottom: "block1_EltAdd_stage2"
  top: "block2_EltAdd_stage2"
}

#-------------------block3--------------------
layer {
  name: "conv1_block3_stage2"
  type: "Convolution"
  bottom: "block2_EltAdd_stage2"
  top: "conv1_block3_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block3_stage2"
    type: "BatchNorm"
    bottom: "conv1_block3_stage2"
    top: "conv1_block3_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block3_stage2"
    type: "Scale"
    bottom: "conv1_block3_stage2"
    top: "conv1_block3_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block3_stage2"
  type: "ReLU"
  bottom: "conv1_block3_stage2"
  top: "conv1_block3_stage2"
}


layer {
  name: "conv2_block3_stage2"
  type: "Convolution"
  bottom: "conv1_block3_stage2"
  top: "conv2_block3_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block3_stage2"
    type: "BatchNorm"
    bottom: "conv2_block3_stage2"
    top: "conv2_block3_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block3_stage2"
    type: "Scale"
    bottom: "conv2_block3_stage2"
    top: "conv2_block3_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block3_stage2"
  type: "ReLU"
  bottom: "conv2_block3_stage2"
  top: "conv2_block3_stage2"
}

layer {
  name: "conv3_block3_stage2"
  type: "Convolution"
  bottom: "conv2_block3_stage2"
  top: "conv3_block3_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block3_stage2"
    type: "BatchNorm"
    bottom: "conv3_block3_stage2"
    top: "conv3_block3_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block3_stage2"
    type: "Scale"
    bottom: "conv3_block3_stage2"
    top: "conv3_block3_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block3_stage2"
  type: "ReLU"
  bottom: "conv3_block3_stage2"
  top: "conv3_block3_stage2"
}

layer {
  name: "concat_block3_stage2"
  type: "Concat"
  bottom: "conv1_block3_stage2"
  bottom: "conv2_block3_stage2"
  bottom: "conv3_block3_stage2"
  top: "concat_block3_stage2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block3_stage2"
  type: "Eltwise"
  bottom: "concat_block3_stage2"
  bottom: "block2_EltAdd_stage2"
  top: "block3_EltAdd_stage2"
}

#---------------------block4----------------------------
layer {
  name: "conv1_block4_stage2"
  type: "Convolution"
  bottom: "block3_EltAdd_stage2"
  top: "conv1_block4_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block4_stage2"
    type: "BatchNorm"
    bottom: "conv1_block4_stage2"
    top: "conv1_block4_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block4_stage2"
    type: "Scale"
    bottom: "conv1_block4_stage2"
    top: "conv1_block4_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block4_stage2"
  type: "ReLU"
  bottom: "conv1_block4_stage2"
  top: "conv1_block4_stage2"
}


layer {
  name: "conv2_block4_stage2"
  type: "Convolution"
  bottom: "conv1_block4_stage2"
  top: "conv2_block4_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block4_stage2"
    type: "BatchNorm"
    bottom: "conv2_block4_stage2"
    top: "conv2_block4_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block4_stage2"
    type: "Scale"
    bottom: "conv2_block4_stage2"
    top: "conv2_block4_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block4_stage2"
  type: "ReLU"
  bottom: "conv2_block4_stage2"
  top: "conv2_block4_stage2"
}

layer {
  name: "conv3_block4_stage2"
  type: "Convolution"
  bottom: "conv2_block4_stage2"
  top: "conv3_block4_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block4_stage2"
    type: "BatchNorm"
    bottom: "conv3_block4_stage2"
    top: "conv3_block4_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block4_stage2"
    type: "Scale"
    bottom: "conv3_block4_stage2"
    top: "conv3_block4_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block4_stage2"
  type: "ReLU"
  bottom: "conv3_block4_stage2"
  top: "conv3_block4_stage2"
}


layer {
  name: "concat_block4_stage2"
  type: "Concat"
  bottom: "conv1_block4_stage2"
  bottom: "conv2_block4_stage2"
  bottom: "conv3_block4_stage2"
  top: "concat_block4_stage2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block4_stage2"
  type: "Eltwise"
  bottom: "concat_block4_stage2"
  bottom: "block3_EltAdd_stage2"
  top: "block4_EltAdd_stage2"
}

#----------------------block5------------------
layer {
  name: "conv1_block5_stage2"
  type: "Convolution"
  bottom: "block4_EltAdd_stage2"
  top: "conv1_block5_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block5_stage2"
    type: "BatchNorm"
    bottom: "conv1_block5_stage2"
    top: "conv1_block5_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block5_stage2"
    type: "Scale"
    bottom: "conv1_block5_stage2"
    top: "conv1_block5_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block5_stage2"
  type: "ReLU"
  bottom: "conv1_block5_stage2"
  top: "conv1_block5_stage2"
}


layer {
  name: "conv2_block5_stage2"
  type: "Convolution"
  bottom: "conv1_block5_stage2"
  top: "conv2_block5_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block5_stage2"
    type: "BatchNorm"
    bottom: "conv2_block5_stage2"
    top: "conv2_block5_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block5_stage2"
    type: "Scale"
    bottom: "conv2_block5_stage2"
    top: "conv2_block5_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block5_stage2"
  type: "ReLU"
  bottom: "conv2_block5_stage2"
  top: "conv2_block5_stage2"
}

layer {
  name: "conv3_block5_stage2"
  type: "Convolution"
  bottom: "conv2_block5_stage2"
  top: "conv3_block5_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block5_stage2"
    type: "BatchNorm"
    bottom: "conv3_block5_stage2"
    top: "conv3_block5_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block5_stage2"
    type: "Scale"
    bottom: "conv3_block5_stage2"
    top: "conv3_block5_stage2"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block5_stage2"
  type: "ReLU"
  bottom: "conv3_block5_stage2"
  top: "conv3_block5_stage2"
}


layer {
  name: "concat_block5_stage2"
  type: "Concat"
  bottom: "conv1_block5_stage2"
  bottom: "conv2_block5_stage2"
  bottom: "conv3_block5_stage2"
  top: "concat_block5_stage2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block5_stage2"
  type: "Eltwise"
  bottom: "concat_block5_stage2"
  bottom: "block4_EltAdd_stage2"
  top: "block5_EltAdd_stage2"
}

layer {
  name: "conv1_stage2"
  type: "Convolution"
  bottom: "block5_EltAdd_stage2"
  top: "conv1_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_stage2"
    type: "BatchNorm"
    bottom: "conv1_stage2"
    top: "conv1_stage2" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_stage2"
    type: "Scale"
    bottom: "conv1_stage2"
    top: "conv1_stage2"
    scale_param {
        bias_term: true
    }
}
layer {
  name: "relu1_stage2"
  type: "ReLU"
  bottom: "conv1_stage2"
  top: "conv1_stage2"
}

layer {
  name: "conv2_stage2"
  type: "Convolution"
  bottom: "conv1_stage2"
  top: "conv2_stage2"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 8.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 15
    pad: 0
    kernel_size: 1
    weight_filler {
     type: "msra"
    }
  }
}

layer {
  name: "loss_stage2"
  type: "EuclideanLoss"
  bottom: "conv2_stage2"
  bottom: "label_lower"
  top: "loss_stage2"
  loss_weight:0.6
}

#################################################################################
#--------------------------stage 3-----------------------------------------------
layer {
  name: "concat_stage3"
  type: "Concat"
  bottom: "conv4_4_CPM"
  bottom: "conv2_stage2"
  bottom: "pool_center_lower"
  top: "concat_stage3"
  concat_param {
    axis: 1
  }
}

layer {
  name: "conv_stage3"
  type: "Convolution"
  bottom: "concat_stage3"
  top: "conv_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv_stage3"
    type: "BatchNorm"
    bottom: "conv_stage3"
    top: "conv_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1"
    type: "Scale"
    bottom: "conv_stage3"
    top: "conv_stage3"
    scale_param {
        bias_term: true
    }
}
layer {
  name: "relu_stage3"
  type: "ReLU"
  bottom: "conv_stage3"
  top: "conv_stage3"
}

#------------------block1----------------------
layer {
  name: "conv1_block1_stage3"
  type: "Convolution"
  bottom: "conv_stage3"
  top: "conv1_block1_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block1_stage3"
    type: "BatchNorm"
    bottom: "conv1_block1_stage3"
    top: "conv1_block1_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block1_stage3"
    type: "Scale"
    bottom: "conv1_block1_stage3"
    top: "conv1_block1_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block1_stage3"
  type: "ReLU"
  bottom: "conv1_block1_stage3"
  top: "conv1_block1_stage3"
}


layer {
  name: "conv2_block1_stage3"
  type: "Convolution"
  bottom: "conv1_block1_stage3"
  top: "conv2_block1_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block1_stage3"
    type: "BatchNorm"
    bottom: "conv2_block1_stage3"
    top: "conv2_block1_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block1_stage3"
    type: "Scale"
    bottom: "conv2_block1_stage3"
    top: "conv2_block1_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block1_stage3"
  type: "ReLU"
  bottom: "conv2_block1_stage3"
  top: "conv2_block1_stage3"
}

layer {
  name: "conv3_block1_stage3"
  type: "Convolution"
  bottom: "conv2_block1_stage3"
  top: "conv3_block1_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block1_stage3"
    type: "BatchNorm"
    bottom: "conv3_block1_stage3"
    top: "conv3_block1_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block1_stage3"
    type: "Scale"
    bottom: "conv3_block1_stage3"
    top: "conv3_block1_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block1_stage3"
  type: "ReLU"
  bottom: "conv3_block1_stage3"
  top: "conv3_block1_stage3"
}


layer {
  name: "concat_block1_stage3"
  type: "Concat"
  bottom: "conv1_block1_stage3"
  bottom: "conv2_block1_stage3"
  bottom: "conv3_block1_stage3"
  top: "concat_block1_stage3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block1_stage3"
  type: "Eltwise"
  bottom: "conv_stage3"
  bottom: "concat_block1_stage3"
  top: "block1_EltAdd_stage3"
}

#--------------block2----------------------------------
layer {
  name: "conv1_block2_stage3"
  type: "Convolution"
  bottom: "block1_EltAdd_stage3"
  top: "conv1_block2_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block2_stage3"
    type: "BatchNorm"
    bottom: "conv1_block2_stage3"
    top: "conv1_block2_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block2_stage3"
    type: "Scale"
    bottom: "conv1_block2_stage3"
    top: "conv1_block2_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block2_stage3"
  type: "ReLU"
  bottom: "conv1_block2_stage3"
  top: "conv1_block2_stage3"
}


layer {
  name: "conv2_block2_stage3"
  type: "Convolution"
  bottom: "conv1_block2_stage3"
  top: "conv2_block2_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block2_stage3"
    type: "BatchNorm"
    bottom: "conv2_block2_stage3"
    top: "conv2_block2_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block2_stage3"
    type: "Scale"
    bottom: "conv2_block2_stage3"
    top: "conv2_block2_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block2_stage3"
  type: "ReLU"
  bottom: "conv2_block2_stage3"
  top: "conv2_block2_stage3"
}

layer {
  name: "conv3_block2_stage3"
  type: "Convolution"
  bottom: "conv2_block2_stage3"
  top: "conv3_block2_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block2_stage3"
    type: "BatchNorm"
    bottom: "conv3_block2_stage3"
    top: "conv3_block2_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block2_stage3"
    type: "Scale"
    bottom: "conv3_block2_stage3"
    top: "conv3_block2_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block2_stage3"
  type: "ReLU"
  bottom: "conv3_block2_stage3"
  top: "conv3_block2_stage3"
}


layer {
  name: "concat_block2_stage3"
  type: "Concat"
  bottom: "conv1_block2_stage3"
  bottom: "conv2_block2_stage3"
  bottom: "conv3_block2_stage3"
  top: "concat_block2_stage3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block2_stage3"
  type: "Eltwise"
  bottom: "concat_block2_stage3"
  bottom: "block1_EltAdd_stage3"
  top: "block2_EltAdd_stage3"
}

#-------------------block3--------------------
layer {
  name: "conv1_block3_stage3"
  type: "Convolution"
  bottom: "block2_EltAdd_stage3"
  top: "conv1_block3_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block3_stage3"
    type: "BatchNorm"
    bottom: "conv1_block3_stage3"
    top: "conv1_block3_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block3_stage3"
    type: "Scale"
    bottom: "conv1_block3_stage3"
    top: "conv1_block3_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block3_stage3"
  type: "ReLU"
  bottom: "conv1_block3_stage3"
  top: "conv1_block3_stage3"
}


layer {
  name: "conv2_block3_stage3"
  type: "Convolution"
  bottom: "conv1_block3_stage3"
  top: "conv2_block3_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block3_stage3"
    type: "BatchNorm"
    bottom: "conv2_block3_stage3"
    top: "conv2_block3_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block3_stage3"
    type: "Scale"
    bottom: "conv2_block3_stage3"
    top: "conv2_block3_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block3_stage3"
  type: "ReLU"
  bottom: "conv2_block3_stage3"
  top: "conv2_block3_stage3"
}

layer {
  name: "conv3_block3_stage3"
  type: "Convolution"
  bottom: "conv2_block3_stage3"
  top: "conv3_block3_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block3_stage3"
    type: "BatchNorm"
    bottom: "conv3_block3_stage3"
    top: "conv3_block3_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block3_stage3"
    type: "Scale"
    bottom: "conv3_block3_stage3"
    top: "conv3_block3_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block3_stage3"
  type: "ReLU"
  bottom: "conv3_block3_stage3"
  top: "conv3_block3_stage3"
}

layer {
  name: "concat_block3_stage3"
  type: "Concat"
  bottom: "conv1_block3_stage3"
  bottom: "conv2_block3_stage3"
  bottom: "conv3_block3_stage3"
  top: "concat_block3_stage3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block3_stage3"
  type: "Eltwise"
  bottom: "concat_block3_stage3"
  bottom: "block2_EltAdd_stage3"
  top: "block3_EltAdd_stage3"
}

#---------------------block4----------------------------
layer {
  name: "conv1_block4_stage3"
  type: "Convolution"
  bottom: "block3_EltAdd_stage3"
  top: "conv1_block4_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block4_stage3"
    type: "BatchNorm"
    bottom: "conv1_block4_stage3"
    top: "conv1_block4_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block4_stage3"
    type: "Scale"
    bottom: "conv1_block4_stage3"
    top: "conv1_block4_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block4_stage3"
  type: "ReLU"
  bottom: "conv1_block4_stage3"
  top: "conv1_block4_stage3"
}


layer {
  name: "conv2_block4_stage3"
  type: "Convolution"
  bottom: "conv1_block4_stage3"
  top: "conv2_block4_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block4_stage3"
    type: "BatchNorm"
    bottom: "conv2_block4_stage3"
    top: "conv2_block4_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block4_stage3"
    type: "Scale"
    bottom: "conv2_block4_stage3"
    top: "conv2_block4_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block4_stage3"
  type: "ReLU"
  bottom: "conv2_block4_stage3"
  top: "conv2_block4_stage3"
}

layer {
  name: "conv3_block4_stage3"
  type: "Convolution"
  bottom: "conv2_block4_stage3"
  top: "conv3_block4_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block4_stage3"
    type: "BatchNorm"
    bottom: "conv3_block4_stage3"
    top: "conv3_block4_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block4_stage3"
    type: "Scale"
    bottom: "conv3_block4_stage3"
    top: "conv3_block4_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block4_stage3"
  type: "ReLU"
  bottom: "conv3_block4_stage3"
  top: "conv3_block4_stage3"
}


layer {
  name: "concat_block4_stage3"
  type: "Concat"
  bottom: "conv1_block4_stage3"
  bottom: "conv2_block4_stage3"
  bottom: "conv3_block4_stage3"
  top: "concat_block4_stage3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block4_stage3"
  type: "Eltwise"
  bottom: "concat_block4_stage3"
  bottom: "block3_EltAdd_stage3"
  top: "block4_EltAdd_stage3"
}

#----------------------block5------------------
layer {
  name: "conv1_block5_stage3"
  type: "Convolution"
  bottom: "block4_EltAdd_stage3"
  top: "conv1_block5_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block5_stage3"
    type: "BatchNorm"
    bottom: "conv1_block5_stage3"
    top: "conv1_block5_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block5_stage3"
    type: "Scale"
    bottom: "conv1_block5_stage3"
    top: "conv1_block5_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block5_stage3"
  type: "ReLU"
  bottom: "conv1_block5_stage3"
  top: "conv1_block5_stage3"
}


layer {
  name: "conv2_block5_stage3"
  type: "Convolution"
  bottom: "conv1_block5_stage3"
  top: "conv2_block5_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block5_stage3"
    type: "BatchNorm"
    bottom: "conv2_block5_stage3"
    top: "conv2_block5_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block5_stage3"
    type: "Scale"
    bottom: "conv2_block5_stage3"
    top: "conv2_block5_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block5_stage3"
  type: "ReLU"
  bottom: "conv2_block5_stage3"
  top: "conv2_block5_stage3"
}

layer {
  name: "conv3_block5_stage3"
  type: "Convolution"
  bottom: "conv2_block5_stage3"
  top: "conv3_block5_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block5_stage3"
    type: "BatchNorm"
    bottom: "conv3_block5_stage3"
    top: "conv3_block5_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block5_stage3"
    type: "Scale"
    bottom: "conv3_block5_stage3"
    top: "conv3_block5_stage3"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block5_stage3"
  type: "ReLU"
  bottom: "conv3_block5_stage3"
  top: "conv3_block5_stage3"
}


layer {
  name: "concat_block5_stage3"
  type: "Concat"
  bottom: "conv1_block5_stage3"
  bottom: "conv2_block5_stage3"
  bottom: "conv3_block5_stage3"
  top: "concat_block5_stage3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block5_stage3"
  type: "Eltwise"
  bottom: "concat_block5_stage3"
  bottom: "block4_EltAdd_stage3"
  top: "block5_EltAdd_stage3"
}

layer {
  name: "conv1_stage3"
  type: "Convolution"
  bottom: "block5_EltAdd_stage3"
  top: "conv1_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_stage3"
    type: "BatchNorm"
    bottom: "conv1_stage3"
    top: "conv1_stage3" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_stage3"
    type: "Scale"
    bottom: "conv1_stage3"
    top: "conv1_stage3"
    scale_param {
        bias_term: true
    }
}
layer {
  name: "relu1_stage3"
  type: "ReLU"
  bottom: "conv1_stage3"
  top: "conv1_stage3"
}

layer {
  name: "conv2_stage3"
  type: "Convolution"
  bottom: "conv1_stage3"
  top: "conv2_stage3"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 8.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 15
    pad: 0
    kernel_size: 1
    weight_filler {
     type: "msra"
    }
  }
}

layer {
  name: "loss_stage3"
  type: "EuclideanLoss"
  bottom: "conv2_stage3"
  bottom: "label_lower"
  top: "loss_stage3"
  loss_weight:0.7
}

#################################################################################
#--------------------------stage 4-----------------------------------------------
layer {
  name: "concat_stage4"
  type: "Concat"
  bottom: "conv4_4_CPM"
  bottom: "conv2_stage3"
  bottom: "pool_center_lower"
  top: "concat_stage4"
  concat_param {
    axis: 1
  }
}

layer {
  name: "conv_stage4"
  type: "Convolution"
  bottom: "concat_stage4"
  top: "conv_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv_stage4"
    type: "BatchNorm"
    bottom: "conv_stage4"
    top: "conv_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1"
    type: "Scale"
    bottom: "conv_stage4"
    top: "conv_stage4"
    scale_param {
        bias_term: true
    }
}
layer {
  name: "relu_stage4"
  type: "ReLU"
  bottom: "conv_stage4"
  top: "conv_stage4"
}

#------------------block1----------------------
layer {
  name: "conv1_block1_stage4"
  type: "Convolution"
  bottom: "conv_stage4"
  top: "conv1_block1_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block1_stage4"
    type: "BatchNorm"
    bottom: "conv1_block1_stage4"
    top: "conv1_block1_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block1_stage4"
    type: "Scale"
    bottom: "conv1_block1_stage4"
    top: "conv1_block1_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block1_stage4"
  type: "ReLU"
  bottom: "conv1_block1_stage4"
  top: "conv1_block1_stage4"
}


layer {
  name: "conv2_block1_stage4"
  type: "Convolution"
  bottom: "conv1_block1_stage4"
  top: "conv2_block1_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block1_stage4"
    type: "BatchNorm"
    bottom: "conv2_block1_stage4"
    top: "conv2_block1_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block1_stage4"
    type: "Scale"
    bottom: "conv2_block1_stage4"
    top: "conv2_block1_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block1_stage4"
  type: "ReLU"
  bottom: "conv2_block1_stage4"
  top: "conv2_block1_stage4"
}

layer {
  name: "conv3_block1_stage4"
  type: "Convolution"
  bottom: "conv2_block1_stage4"
  top: "conv3_block1_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block1_stage4"
    type: "BatchNorm"
    bottom: "conv3_block1_stage4"
    top: "conv3_block1_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block1_stage4"
    type: "Scale"
    bottom: "conv3_block1_stage4"
    top: "conv3_block1_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block1_stage4"
  type: "ReLU"
  bottom: "conv3_block1_stage4"
  top: "conv3_block1_stage4"
}


layer {
  name: "concat_block1_stage4"
  type: "Concat"
  bottom: "conv1_block1_stage4"
  bottom: "conv2_block1_stage4"
  bottom: "conv3_block1_stage4"
  top: "concat_block1_stage4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block1_stage4"
  type: "Eltwise"
  bottom: "conv_stage4"
  bottom: "concat_block1_stage4"
  top: "block1_EltAdd_stage4"
}

#--------------block2----------------------------------
layer {
  name: "conv1_block2_stage4"
  type: "Convolution"
  bottom: "block1_EltAdd_stage4"
  top: "conv1_block2_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block2_stage4"
    type: "BatchNorm"
    bottom: "conv1_block2_stage4"
    top: "conv1_block2_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block2_stage4"
    type: "Scale"
    bottom: "conv1_block2_stage4"
    top: "conv1_block2_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block2_stage4"
  type: "ReLU"
  bottom: "conv1_block2_stage4"
  top: "conv1_block2_stage4"
}


layer {
  name: "conv2_block2_stage4"
  type: "Convolution"
  bottom: "conv1_block2_stage4"
  top: "conv2_block2_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block2_stage4"
    type: "BatchNorm"
    bottom: "conv2_block2_stage4"
    top: "conv2_block2_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block2_stage4"
    type: "Scale"
    bottom: "conv2_block2_stage4"
    top: "conv2_block2_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block2_stage4"
  type: "ReLU"
  bottom: "conv2_block2_stage4"
  top: "conv2_block2_stage4"
}

layer {
  name: "conv3_block2_stage4"
  type: "Convolution"
  bottom: "conv2_block2_stage4"
  top: "conv3_block2_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block2_stage4"
    type: "BatchNorm"
    bottom: "conv3_block2_stage4"
    top: "conv3_block2_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block2_stage4"
    type: "Scale"
    bottom: "conv3_block2_stage4"
    top: "conv3_block2_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block2_stage4"
  type: "ReLU"
  bottom: "conv3_block2_stage4"
  top: "conv3_block2_stage4"
}


layer {
  name: "concat_block2_stage4"
  type: "Concat"
  bottom: "conv1_block2_stage4"
  bottom: "conv2_block2_stage4"
  bottom: "conv3_block2_stage4"
  top: "concat_block2_stage4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block2_stage4"
  type: "Eltwise"
  bottom: "concat_block2_stage4"
  bottom: "block1_EltAdd_stage4"
  top: "block2_EltAdd_stage4"
}

#-------------------block3--------------------
layer {
  name: "conv1_block3_stage4"
  type: "Convolution"
  bottom: "block2_EltAdd_stage4"
  top: "conv1_block3_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block3_stage4"
    type: "BatchNorm"
    bottom: "conv1_block3_stage4"
    top: "conv1_block3_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block3_stage4"
    type: "Scale"
    bottom: "conv1_block3_stage4"
    top: "conv1_block3_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block3_stage4"
  type: "ReLU"
  bottom: "conv1_block3_stage4"
  top: "conv1_block3_stage4"
}


layer {
  name: "conv2_block3_stage4"
  type: "Convolution"
  bottom: "conv1_block3_stage4"
  top: "conv2_block3_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block3_stage4"
    type: "BatchNorm"
    bottom: "conv2_block3_stage4"
    top: "conv2_block3_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block3_stage4"
    type: "Scale"
    bottom: "conv2_block3_stage4"
    top: "conv2_block3_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block3_stage4"
  type: "ReLU"
  bottom: "conv2_block3_stage4"
  top: "conv2_block3_stage4"
}

layer {
  name: "conv3_block3_stage4"
  type: "Convolution"
  bottom: "conv2_block3_stage4"
  top: "conv3_block3_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block3_stage4"
    type: "BatchNorm"
    bottom: "conv3_block3_stage4"
    top: "conv3_block3_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block3_stage4"
    type: "Scale"
    bottom: "conv3_block3_stage4"
    top: "conv3_block3_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block3_stage4"
  type: "ReLU"
  bottom: "conv3_block3_stage4"
  top: "conv3_block3_stage4"
}

layer {
  name: "concat_block3_stage4"
  type: "Concat"
  bottom: "conv1_block3_stage4"
  bottom: "conv2_block3_stage4"
  bottom: "conv3_block3_stage4"
  top: "concat_block3_stage4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block3_stage4"
  type: "Eltwise"
  bottom: "concat_block3_stage4"
  bottom: "block2_EltAdd_stage4"
  top: "block3_EltAdd_stage4"
}

#---------------------block4----------------------------
layer {
  name: "conv1_block4_stage4"
  type: "Convolution"
  bottom: "block3_EltAdd_stage4"
  top: "conv1_block4_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block4_stage4"
    type: "BatchNorm"
    bottom: "conv1_block4_stage4"
    top: "conv1_block4_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block4_stage4"
    type: "Scale"
    bottom: "conv1_block4_stage4"
    top: "conv1_block4_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block4_stage4"
  type: "ReLU"
  bottom: "conv1_block4_stage4"
  top: "conv1_block4_stage4"
}


layer {
  name: "conv2_block4_stage4"
  type: "Convolution"
  bottom: "conv1_block4_stage4"
  top: "conv2_block4_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block4_stage4"
    type: "BatchNorm"
    bottom: "conv2_block4_stage4"
    top: "conv2_block4_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block4_stage4"
    type: "Scale"
    bottom: "conv2_block4_stage4"
    top: "conv2_block4_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block4_stage4"
  type: "ReLU"
  bottom: "conv2_block4_stage4"
  top: "conv2_block4_stage4"
}

layer {
  name: "conv3_block4_stage4"
  type: "Convolution"
  bottom: "conv2_block4_stage4"
  top: "conv3_block4_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block4_stage4"
    type: "BatchNorm"
    bottom: "conv3_block4_stage4"
    top: "conv3_block4_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block4_stage4"
    type: "Scale"
    bottom: "conv3_block4_stage4"
    top: "conv3_block4_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block4_stage4"
  type: "ReLU"
  bottom: "conv3_block4_stage4"
  top: "conv3_block4_stage4"
}


layer {
  name: "concat_block4_stage4"
  type: "Concat"
  bottom: "conv1_block4_stage4"
  bottom: "conv2_block4_stage4"
  bottom: "conv3_block4_stage4"
  top: "concat_block4_stage4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block4_stage4"
  type: "Eltwise"
  bottom: "concat_block4_stage4"
  bottom: "block3_EltAdd_stage4"
  top: "block4_EltAdd_stage4"
}

#----------------------block5------------------
layer {
  name: "conv1_block5_stage4"
  type: "Convolution"
  bottom: "block4_EltAdd_stage4"
  top: "conv1_block5_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block5_stage4"
    type: "BatchNorm"
    bottom: "conv1_block5_stage4"
    top: "conv1_block5_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block5_stage4"
    type: "Scale"
    bottom: "conv1_block5_stage4"
    top: "conv1_block5_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block5_stage4"
  type: "ReLU"
  bottom: "conv1_block5_stage4"
  top: "conv1_block5_stage4"
}


layer {
  name: "conv2_block5_stage4"
  type: "Convolution"
  bottom: "conv1_block5_stage4"
  top: "conv2_block5_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block5_stage4"
    type: "BatchNorm"
    bottom: "conv2_block5_stage4"
    top: "conv2_block5_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block5_stage4"
    type: "Scale"
    bottom: "conv2_block5_stage4"
    top: "conv2_block5_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block5_stage4"
  type: "ReLU"
  bottom: "conv2_block5_stage4"
  top: "conv2_block5_stage4"
}

layer {
  name: "conv3_block5_stage4"
  type: "Convolution"
  bottom: "conv2_block5_stage4"
  top: "conv3_block5_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block5_stage4"
    type: "BatchNorm"
    bottom: "conv3_block5_stage4"
    top: "conv3_block5_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block5_stage4"
    type: "Scale"
    bottom: "conv3_block5_stage4"
    top: "conv3_block5_stage4"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block5_stage4"
  type: "ReLU"
  bottom: "conv3_block5_stage4"
  top: "conv3_block5_stage4"
}


layer {
  name: "concat_block5_stage4"
  type: "Concat"
  bottom: "conv1_block5_stage4"
  bottom: "conv2_block5_stage4"
  bottom: "conv3_block5_stage4"
  top: "concat_block5_stage4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block5_stage4"
  type: "Eltwise"
  bottom: "concat_block5_stage4"
  bottom: "block4_EltAdd_stage4"
  top: "block5_EltAdd_stage4"
}

layer {
  name: "conv1_stage4"
  type: "Convolution"
  bottom: "block5_EltAdd_stage4"
  top: "conv1_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_stage4"
    type: "BatchNorm"
    bottom: "conv1_stage4"
    top: "conv1_stage4" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_stage4"
    type: "Scale"
    bottom: "conv1_stage4"
    top: "conv1_stage4"
    scale_param {
        bias_term: true
    }
}
layer {
  name: "relu1_stage4"
  type: "ReLU"
  bottom: "conv1_stage4"
  top: "conv1_stage4"
}

layer {
  name: "conv2_stage4"
  type: "Convolution"
  bottom: "conv1_stage4"
  top: "conv2_stage4"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 8.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 15
    pad: 0
    kernel_size: 1
    weight_filler {
     type: "msra"
    }
  }
}

layer {
  name: "loss_stage4"
  type: "EuclideanLoss"
  bottom: "conv2_stage4"
  bottom: "label_lower"
  top: "loss_stage4"
  loss_weight:0.8
}

#################################################################################
#--------------------------stage 5-----------------------------------------------
layer {
  name: "concat_stage5"
  type: "Concat"
  bottom: "conv4_4_CPM"
  bottom: "conv2_stage4"
  bottom: "pool_center_lower"
  top: "concat_stage5"
  concat_param {
    axis: 1
  }
}

layer {
  name: "conv_stage5"
  type: "Convolution"
  bottom: "concat_stage5"
  top: "conv_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv_stage5"
    type: "BatchNorm"
    bottom: "conv_stage5"
    top: "conv_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1"
    type: "Scale"
    bottom: "conv_stage5"
    top: "conv_stage5"
    scale_param {
        bias_term: true
    }
}
layer {
  name: "relu_stage5"
  type: "ReLU"
  bottom: "conv_stage5"
  top: "conv_stage5"
}

#------------------block1----------------------
layer {
  name: "conv1_block1_stage5"
  type: "Convolution"
  bottom: "conv_stage5"
  top: "conv1_block1_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block1_stage5"
    type: "BatchNorm"
    bottom: "conv1_block1_stage5"
    top: "conv1_block1_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block1_stage5"
    type: "Scale"
    bottom: "conv1_block1_stage5"
    top: "conv1_block1_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block1_stage5"
  type: "ReLU"
  bottom: "conv1_block1_stage5"
  top: "conv1_block1_stage5"
}


layer {
  name: "conv2_block1_stage5"
  type: "Convolution"
  bottom: "conv1_block1_stage5"
  top: "conv2_block1_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block1_stage5"
    type: "BatchNorm"
    bottom: "conv2_block1_stage5"
    top: "conv2_block1_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block1_stage5"
    type: "Scale"
    bottom: "conv2_block1_stage5"
    top: "conv2_block1_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block1_stage5"
  type: "ReLU"
  bottom: "conv2_block1_stage5"
  top: "conv2_block1_stage5"
}

layer {
  name: "conv3_block1_stage5"
  type: "Convolution"
  bottom: "conv2_block1_stage5"
  top: "conv3_block1_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block1_stage5"
    type: "BatchNorm"
    bottom: "conv3_block1_stage5"
    top: "conv3_block1_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block1_stage5"
    type: "Scale"
    bottom: "conv3_block1_stage5"
    top: "conv3_block1_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block1_stage5"
  type: "ReLU"
  bottom: "conv3_block1_stage5"
  top: "conv3_block1_stage5"
}


layer {
  name: "concat_block1_stage5"
  type: "Concat"
  bottom: "conv1_block1_stage5"
  bottom: "conv2_block1_stage5"
  bottom: "conv3_block1_stage5"
  top: "concat_block1_stage5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block1_stage5"
  type: "Eltwise"
  bottom: "conv_stage5"
  bottom: "concat_block1_stage5"
  top: "block1_EltAdd_stage5"
}

#--------------block2----------------------------------
layer {
  name: "conv1_block2_stage5"
  type: "Convolution"
  bottom: "block1_EltAdd_stage5"
  top: "conv1_block2_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block2_stage5"
    type: "BatchNorm"
    bottom: "conv1_block2_stage5"
    top: "conv1_block2_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block2_stage5"
    type: "Scale"
    bottom: "conv1_block2_stage5"
    top: "conv1_block2_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block2_stage5"
  type: "ReLU"
  bottom: "conv1_block2_stage5"
  top: "conv1_block2_stage5"
}


layer {
  name: "conv2_block2_stage5"
  type: "Convolution"
  bottom: "conv1_block2_stage5"
  top: "conv2_block2_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block2_stage5"
    type: "BatchNorm"
    bottom: "conv2_block2_stage5"
    top: "conv2_block2_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block2_stage5"
    type: "Scale"
    bottom: "conv2_block2_stage5"
    top: "conv2_block2_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block2_stage5"
  type: "ReLU"
  bottom: "conv2_block2_stage5"
  top: "conv2_block2_stage5"
}

layer {
  name: "conv3_block2_stage5"
  type: "Convolution"
  bottom: "conv2_block2_stage5"
  top: "conv3_block2_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block2_stage5"
    type: "BatchNorm"
    bottom: "conv3_block2_stage5"
    top: "conv3_block2_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block2_stage5"
    type: "Scale"
    bottom: "conv3_block2_stage5"
    top: "conv3_block2_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block2_stage5"
  type: "ReLU"
  bottom: "conv3_block2_stage5"
  top: "conv3_block2_stage5"
}


layer {
  name: "concat_block2_stage5"
  type: "Concat"
  bottom: "conv1_block2_stage5"
  bottom: "conv2_block2_stage5"
  bottom: "conv3_block2_stage5"
  top: "concat_block2_stage5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block2_stage5"
  type: "Eltwise"
  bottom: "concat_block2_stage5"
  bottom: "block1_EltAdd_stage5"
  top: "block2_EltAdd_stage5"
}

#-------------------block3--------------------
layer {
  name: "conv1_block3_stage5"
  type: "Convolution"
  bottom: "block2_EltAdd_stage5"
  top: "conv1_block3_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block3_stage5"
    type: "BatchNorm"
    bottom: "conv1_block3_stage5"
    top: "conv1_block3_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block3_stage5"
    type: "Scale"
    bottom: "conv1_block3_stage5"
    top: "conv1_block3_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block3_stage5"
  type: "ReLU"
  bottom: "conv1_block3_stage5"
  top: "conv1_block3_stage5"
}


layer {
  name: "conv2_block3_stage5"
  type: "Convolution"
  bottom: "conv1_block3_stage5"
  top: "conv2_block3_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block3_stage5"
    type: "BatchNorm"
    bottom: "conv2_block3_stage5"
    top: "conv2_block3_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block3_stage5"
    type: "Scale"
    bottom: "conv2_block3_stage5"
    top: "conv2_block3_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block3_stage5"
  type: "ReLU"
  bottom: "conv2_block3_stage5"
  top: "conv2_block3_stage5"
}

layer {
  name: "conv3_block3_stage5"
  type: "Convolution"
  bottom: "conv2_block3_stage5"
  top: "conv3_block3_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block3_stage5"
    type: "BatchNorm"
    bottom: "conv3_block3_stage5"
    top: "conv3_block3_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block3_stage5"
    type: "Scale"
    bottom: "conv3_block3_stage5"
    top: "conv3_block3_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block3_stage5"
  type: "ReLU"
  bottom: "conv3_block3_stage5"
  top: "conv3_block3_stage5"
}

layer {
  name: "concat_block3_stage5"
  type: "Concat"
  bottom: "conv1_block3_stage5"
  bottom: "conv2_block3_stage5"
  bottom: "conv3_block3_stage5"
  top: "concat_block3_stage5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block3_stage5"
  type: "Eltwise"
  bottom: "concat_block3_stage5"
  bottom: "block2_EltAdd_stage5"
  top: "block3_EltAdd_stage5"
}

#---------------------block4----------------------------
layer {
  name: "conv1_block4_stage5"
  type: "Convolution"
  bottom: "block3_EltAdd_stage5"
  top: "conv1_block4_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block4_stage5"
    type: "BatchNorm"
    bottom: "conv1_block4_stage5"
    top: "conv1_block4_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block4_stage5"
    type: "Scale"
    bottom: "conv1_block4_stage5"
    top: "conv1_block4_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block4_stage5"
  type: "ReLU"
  bottom: "conv1_block4_stage5"
  top: "conv1_block4_stage5"
}


layer {
  name: "conv2_block4_stage5"
  type: "Convolution"
  bottom: "conv1_block4_stage5"
  top: "conv2_block4_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block4_stage5"
    type: "BatchNorm"
    bottom: "conv2_block4_stage5"
    top: "conv2_block4_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block4_stage5"
    type: "Scale"
    bottom: "conv2_block4_stage5"
    top: "conv2_block4_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block4_stage5"
  type: "ReLU"
  bottom: "conv2_block4_stage5"
  top: "conv2_block4_stage5"
}

layer {
  name: "conv3_block4_stage5"
  type: "Convolution"
  bottom: "conv2_block4_stage5"
  top: "conv3_block4_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block4_stage5"
    type: "BatchNorm"
    bottom: "conv3_block4_stage5"
    top: "conv3_block4_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block4_stage5"
    type: "Scale"
    bottom: "conv3_block4_stage5"
    top: "conv3_block4_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block4_stage5"
  type: "ReLU"
  bottom: "conv3_block4_stage5"
  top: "conv3_block4_stage5"
}


layer {
  name: "concat_block4_stage5"
  type: "Concat"
  bottom: "conv1_block4_stage5"
  bottom: "conv2_block4_stage5"
  bottom: "conv3_block4_stage5"
  top: "concat_block4_stage5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block4_stage5"
  type: "Eltwise"
  bottom: "concat_block4_stage5"
  bottom: "block3_EltAdd_stage5"
  top: "block4_EltAdd_stage5"
}

#----------------------block5------------------
layer {
  name: "conv1_block5_stage5"
  type: "Convolution"
  bottom: "block4_EltAdd_stage5"
  top: "conv1_block5_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block5_stage5"
    type: "BatchNorm"
    bottom: "conv1_block5_stage5"
    top: "conv1_block5_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block5_stage5"
    type: "Scale"
    bottom: "conv1_block5_stage5"
    top: "conv1_block5_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block5_stage5"
  type: "ReLU"
  bottom: "conv1_block5_stage5"
  top: "conv1_block5_stage5"
}


layer {
  name: "conv2_block5_stage5"
  type: "Convolution"
  bottom: "conv1_block5_stage5"
  top: "conv2_block5_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block5_stage5"
    type: "BatchNorm"
    bottom: "conv2_block5_stage5"
    top: "conv2_block5_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block5_stage5"
    type: "Scale"
    bottom: "conv2_block5_stage5"
    top: "conv2_block5_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block5_stage5"
  type: "ReLU"
  bottom: "conv2_block5_stage5"
  top: "conv2_block5_stage5"
}

layer {
  name: "conv3_block5_stage5"
  type: "Convolution"
  bottom: "conv2_block5_stage5"
  top: "conv3_block5_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block5_stage5"
    type: "BatchNorm"
    bottom: "conv3_block5_stage5"
    top: "conv3_block5_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block5_stage5"
    type: "Scale"
    bottom: "conv3_block5_stage5"
    top: "conv3_block5_stage5"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block5_stage5"
  type: "ReLU"
  bottom: "conv3_block5_stage5"
  top: "conv3_block5_stage5"
}


layer {
  name: "concat_block5_stage5"
  type: "Concat"
  bottom: "conv1_block5_stage5"
  bottom: "conv2_block5_stage5"
  bottom: "conv3_block5_stage5"
  top: "concat_block5_stage5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block5_stage5"
  type: "Eltwise"
  bottom: "concat_block5_stage5"
  bottom: "block4_EltAdd_stage5"
  top: "block5_EltAdd_stage5"
}

layer {
  name: "conv1_stage5"
  type: "Convolution"
  bottom: "block5_EltAdd_stage5"
  top: "conv1_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_stage5"
    type: "BatchNorm"
    bottom: "conv1_stage5"
    top: "conv1_stage5" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_stage5"
    type: "Scale"
    bottom: "conv1_stage5"
    top: "conv1_stage5"
    scale_param {
        bias_term: true
    }
}
layer {
  name: "relu1_stage5"
  type: "ReLU"
  bottom: "conv1_stage5"
  top: "conv1_stage5"
}

layer {
  name: "conv2_stage5"
  type: "Convolution"
  bottom: "conv1_stage5"
  top: "conv2_stage5"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 8.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 15
    pad: 0
    kernel_size: 1
    weight_filler {
     type: "msra"
    }
  }
}

layer {
  name: "loss_stage5"
  type: "EuclideanLoss"
  bottom: "conv2_stage5"
  bottom: "label_lower"
  top: "loss_stage5"
  loss_weight:0.9
}

#################################################################################
#--------------------------stage 6-----------------------------------------------
layer {
  name: "concat_stage6"
  type: "Concat"
  bottom: "conv4_4_CPM"
  bottom: "conv2_stage5"
  bottom: "pool_center_lower"
  top: "concat_stage6"
  concat_param {
    axis: 1
  }
}

layer {
  name: "conv_stage6"
  type: "Convolution"
  bottom: "concat_stage6"
  top: "conv_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv_stage6"
    type: "BatchNorm"
    bottom: "conv_stage6"
    top: "conv_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1"
    type: "Scale"
    bottom: "conv_stage6"
    top: "conv_stage6"
    scale_param {
        bias_term: true
    }
}
layer {
  name: "relu_stage6"
  type: "ReLU"
  bottom: "conv_stage6"
  top: "conv_stage6"
}

#------------------block1----------------------
layer {
  name: "conv1_block1_stage6"
  type: "Convolution"
  bottom: "conv_stage6"
  top: "conv1_block1_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block1_stage6"
    type: "BatchNorm"
    bottom: "conv1_block1_stage6"
    top: "conv1_block1_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block1_stage6"
    type: "Scale"
    bottom: "conv1_block1_stage6"
    top: "conv1_block1_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block1_stage6"
  type: "ReLU"
  bottom: "conv1_block1_stage6"
  top: "conv1_block1_stage6"
}


layer {
  name: "conv2_block1_stage6"
  type: "Convolution"
  bottom: "conv1_block1_stage6"
  top: "conv2_block1_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block1_stage6"
    type: "BatchNorm"
    bottom: "conv2_block1_stage6"
    top: "conv2_block1_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block1_stage6"
    type: "Scale"
    bottom: "conv2_block1_stage6"
    top: "conv2_block1_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block1_stage6"
  type: "ReLU"
  bottom: "conv2_block1_stage6"
  top: "conv2_block1_stage6"
}

layer {
  name: "conv3_block1_stage6"
  type: "Convolution"
  bottom: "conv2_block1_stage6"
  top: "conv3_block1_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block1_stage6"
    type: "BatchNorm"
    bottom: "conv3_block1_stage6"
    top: "conv3_block1_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block1_stage6"
    type: "Scale"
    bottom: "conv3_block1_stage6"
    top: "conv3_block1_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block1_stage6"
  type: "ReLU"
  bottom: "conv3_block1_stage6"
  top: "conv3_block1_stage6"
}


layer {
  name: "concat_block1_stage6"
  type: "Concat"
  bottom: "conv1_block1_stage6"
  bottom: "conv2_block1_stage6"
  bottom: "conv3_block1_stage6"
  top: "concat_block1_stage6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block1_stage6"
  type: "Eltwise"
  bottom: "conv_stage6"
  bottom: "concat_block1_stage6"
  top: "block1_EltAdd_stage6"
}

#--------------block2----------------------------------
layer {
  name: "conv1_block2_stage6"
  type: "Convolution"
  bottom: "block1_EltAdd_stage6"
  top: "conv1_block2_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block2_stage6"
    type: "BatchNorm"
    bottom: "conv1_block2_stage6"
    top: "conv1_block2_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block2_stage6"
    type: "Scale"
    bottom: "conv1_block2_stage6"
    top: "conv1_block2_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block2_stage6"
  type: "ReLU"
  bottom: "conv1_block2_stage6"
  top: "conv1_block2_stage6"
}


layer {
  name: "conv2_block2_stage6"
  type: "Convolution"
  bottom: "conv1_block2_stage6"
  top: "conv2_block2_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block2_stage6"
    type: "BatchNorm"
    bottom: "conv2_block2_stage6"
    top: "conv2_block2_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block2_stage6"
    type: "Scale"
    bottom: "conv2_block2_stage6"
    top: "conv2_block2_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block2_stage6"
  type: "ReLU"
  bottom: "conv2_block2_stage6"
  top: "conv2_block2_stage6"
}

layer {
  name: "conv3_block2_stage6"
  type: "Convolution"
  bottom: "conv2_block2_stage6"
  top: "conv3_block2_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block2_stage6"
    type: "BatchNorm"
    bottom: "conv3_block2_stage6"
    top: "conv3_block2_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block2_stage6"
    type: "Scale"
    bottom: "conv3_block2_stage6"
    top: "conv3_block2_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block2_stage6"
  type: "ReLU"
  bottom: "conv3_block2_stage6"
  top: "conv3_block2_stage6"
}


layer {
  name: "concat_block2_stage6"
  type: "Concat"
  bottom: "conv1_block2_stage6"
  bottom: "conv2_block2_stage6"
  bottom: "conv3_block2_stage6"
  top: "concat_block2_stage6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block2_stage6"
  type: "Eltwise"
  bottom: "concat_block2_stage6"
  bottom: "block1_EltAdd_stage6"
  top: "block2_EltAdd_stage6"
}

#-------------------block3--------------------
layer {
  name: "conv1_block3_stage6"
  type: "Convolution"
  bottom: "block2_EltAdd_stage6"
  top: "conv1_block3_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block3_stage6"
    type: "BatchNorm"
    bottom: "conv1_block3_stage6"
    top: "conv1_block3_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block3_stage6"
    type: "Scale"
    bottom: "conv1_block3_stage6"
    top: "conv1_block3_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block3_stage6"
  type: "ReLU"
  bottom: "conv1_block3_stage6"
  top: "conv1_block3_stage6"
}


layer {
  name: "conv2_block3_stage6"
  type: "Convolution"
  bottom: "conv1_block3_stage6"
  top: "conv2_block3_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block3_stage6"
    type: "BatchNorm"
    bottom: "conv2_block3_stage6"
    top: "conv2_block3_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block3_stage6"
    type: "Scale"
    bottom: "conv2_block3_stage6"
    top: "conv2_block3_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block3_stage6"
  type: "ReLU"
  bottom: "conv2_block3_stage6"
  top: "conv2_block3_stage6"
}

layer {
  name: "conv3_block3_stage6"
  type: "Convolution"
  bottom: "conv2_block3_stage6"
  top: "conv3_block3_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block3_stage6"
    type: "BatchNorm"
    bottom: "conv3_block3_stage6"
    top: "conv3_block3_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block3_stage6"
    type: "Scale"
    bottom: "conv3_block3_stage6"
    top: "conv3_block3_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block3_stage6"
  type: "ReLU"
  bottom: "conv3_block3_stage6"
  top: "conv3_block3_stage6"
}

layer {
  name: "concat_block3_stage6"
  type: "Concat"
  bottom: "conv1_block3_stage6"
  bottom: "conv2_block3_stage6"
  bottom: "conv3_block3_stage6"
  top: "concat_block3_stage6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block3_stage6"
  type: "Eltwise"
  bottom: "concat_block3_stage6"
  bottom: "block2_EltAdd_stage6"
  top: "block3_EltAdd_stage6"
}

#---------------------block4----------------------------
layer {
  name: "conv1_block4_stage6"
  type: "Convolution"
  bottom: "block3_EltAdd_stage6"
  top: "conv1_block4_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block4_stage6"
    type: "BatchNorm"
    bottom: "conv1_block4_stage6"
    top: "conv1_block4_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block4_stage6"
    type: "Scale"
    bottom: "conv1_block4_stage6"
    top: "conv1_block4_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block4_stage6"
  type: "ReLU"
  bottom: "conv1_block4_stage6"
  top: "conv1_block4_stage6"
}


layer {
  name: "conv2_block4_stage6"
  type: "Convolution"
  bottom: "conv1_block4_stage6"
  top: "conv2_block4_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block4_stage6"
    type: "BatchNorm"
    bottom: "conv2_block4_stage6"
    top: "conv2_block4_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block4_stage6"
    type: "Scale"
    bottom: "conv2_block4_stage6"
    top: "conv2_block4_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block4_stage6"
  type: "ReLU"
  bottom: "conv2_block4_stage6"
  top: "conv2_block4_stage6"
}

layer {
  name: "conv3_block4_stage6"
  type: "Convolution"
  bottom: "conv2_block4_stage6"
  top: "conv3_block4_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block4_stage6"
    type: "BatchNorm"
    bottom: "conv3_block4_stage6"
    top: "conv3_block4_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block4_stage6"
    type: "Scale"
    bottom: "conv3_block4_stage6"
    top: "conv3_block4_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block4_stage6"
  type: "ReLU"
  bottom: "conv3_block4_stage6"
  top: "conv3_block4_stage6"
}


layer {
  name: "concat_block4_stage6"
  type: "Concat"
  bottom: "conv1_block4_stage6"
  bottom: "conv2_block4_stage6"
  bottom: "conv3_block4_stage6"
  top: "concat_block4_stage6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block4_stage6"
  type: "Eltwise"
  bottom: "concat_block4_stage6"
  bottom: "block3_EltAdd_stage6"
  top: "block4_EltAdd_stage6"
}

#----------------------block5------------------
layer {
  name: "conv1_block5_stage6"
  type: "Convolution"
  bottom: "block4_EltAdd_stage6"
  top: "conv1_block5_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_block5_stage6"
    type: "BatchNorm"
    bottom: "conv1_block5_stage6"
    top: "conv1_block5_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_block5_stage6"
    type: "Scale"
    bottom: "conv1_block5_stage6"
    top: "conv1_block5_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu1_block5_stage6"
  type: "ReLU"
  bottom: "conv1_block5_stage6"
  top: "conv1_block5_stage6"
}


layer {
  name: "conv2_block5_stage6"
  type: "Convolution"
  bottom: "conv1_block5_stage6"
  top: "conv2_block5_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv2_block5_stage6"
    type: "BatchNorm"
    bottom: "conv2_block5_stage6"
    top: "conv2_block5_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv2_block5_stage6"
    type: "Scale"
    bottom: "conv2_block5_stage6"
    top: "conv2_block5_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu2_block5_stage6"
  type: "ReLU"
  bottom: "conv2_block5_stage6"
  top: "conv2_block5_stage6"
}

layer {
  name: "conv3_block5_stage6"
  type: "Convolution"
  bottom: "conv2_block5_stage6"
  top: "conv3_block5_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv3_block5_stage6"
    type: "BatchNorm"
    bottom: "conv3_block5_stage6"
    top: "conv3_block5_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv3_block5_stage6"
    type: "Scale"
    bottom: "conv3_block5_stage6"
    top: "conv3_block5_stage6"
    scale_param {
        bias_term: true
    }
}

layer {
  name: "relu3_block5_stage6"
  type: "ReLU"
  bottom: "conv3_block5_stage6"
  top: "conv3_block5_stage6"
}


layer {
  name: "concat_block5_stage6"
  type: "Concat"
  bottom: "conv1_block5_stage6"
  bottom: "conv2_block5_stage6"
  bottom: "conv3_block5_stage6"
  top: "concat_block5_stage6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "bypass_block5_stage6"
  type: "Eltwise"
  bottom: "concat_block5_stage6"
  bottom: "block4_EltAdd_stage6"
  top: "block5_EltAdd_stage6"
}

layer {
  name: "conv1_stage6"
  type: "Convolution"
  bottom: "block5_EltAdd_stage6"
  top: "conv1_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
 
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
    bias_term:false
  }
}
layer {
    name: "bn_conv1_stage6"
    type: "BatchNorm"
    bottom: "conv1_stage6"
    top: "conv1_stage6" 
    batch_norm_param {
        use_global_stats: false
    }
}
layer {
    name: "scale_conv1_stage6"
    type: "Scale"
    bottom: "conv1_stage6"
    top: "conv1_stage6"
    scale_param {
        bias_term: true
    }
}
layer {
  name: "relu1_stage6"
  type: "ReLU"
  bottom: "conv1_stage6"
  top: "conv1_stage6"
}

layer {
  name: "conv2_stage6"
  type: "Convolution"
  bottom: "conv1_stage6"
  top: "conv2_stage6"
  param {
    lr_mult: 4.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 8.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 15
    pad: 0
    kernel_size: 1
    weight_filler {
     type: "msra"
    }
  }
}

layer {
  name: "loss_stage6"
  type: "EuclideanLoss"
  bottom: "conv2_stage6"
  bottom: "label_lower"
  top: "loss_stage6"
  loss_weight:1
}
